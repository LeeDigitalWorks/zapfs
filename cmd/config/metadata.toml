# ZapFS Metadata Server Configuration
# All keys use underscores for consistency with CLI flags and environment variables
# Environment variables: ZAPFS_HTTP_PORT, ZAPFS_GRPC_PORT, etc.

# Network
ip = "0.0.0.0"
http_port = 8082
grpc_port = 8083
debug_port = 8085

# TLS (optional)
cert_file = ""
key_file = ""

# Logging
log_level = "info"

# Manager connection
manager_addr = "localhost:8050"

# S3 API
s3_domains = ["localhost"]

# Static Website Hosting
# Domains listed here serve bucket content as static websites.
# Requests to these domains only support GET/HEAD and return HTML errors.
# Example: website_domains = ["s3-website.example.com"]
website_domains = []

# Regional
region_id = "us-west"

# Storage configuration (optional - defaults will be created if not specified)
pools_config = ""
profiles_config = ""
data_dir = "/tmp/zapfs/data"

# Database
# Supported drivers: vitess, mysql (postgres/cockroachdb planned)
db_driver = "vitess"
db_dsn = ""  # e.g., "zapfs:zapfs@tcp(localhost:3306)/zapfs?parseTime=true"
db_max_open_conns = 25
db_max_idle_conns = 5

# Rate Limiting
rate_limit_enabled = true
rate_limit_burst_multiplier = 2

# Redis for distributed rate limiting (optional)
# Enable this when running multiple metadata servers to coordinate rate limits
rate_limit_redis_enabled = false
rate_limit_redis_addr = "localhost:6379"
rate_limit_redis_password = ""
rate_limit_redis_db = 0
rate_limit_redis_pool_size = 10
rate_limit_redis_fail_open = true

# Access Logging (Enterprise: FeatureAuditLog)
# Requires ClickHouse for time-series storage
access_logs_enabled = false
clickhouse_dsn = ""
access_log_batch_size = 10000
access_log_flush_interval = "5s"
access_log_export_interval = "1h"

# Lifecycle Scanner (Community Feature)
# Scans buckets for lifecycle rules and executes expiration/abort actions
# Storage transitions require Enterprise edition
lifecycle_scanner_enabled = false
lifecycle_scan_interval = "1h"        # How often to scan buckets
lifecycle_scan_concurrency = 5        # Parallel bucket processing
lifecycle_scan_batch_size = 1000      # Objects per batch when listing
lifecycle_max_tasks_per_scan = 10000  # Max tasks to enqueue per scan run

# =============================================================================
# Cross-Region Replication Configuration (Enterprise Feature)
# =============================================================================
# Requires FeatureMultiRegion license. These credentials are used by the
# replication handler to authenticate when writing objects to remote regions.
#
# For production, use environment variables:
#   ZAPFS_REPLICATION_ACCESS_KEY_ID
#   ZAPFS_REPLICATION_SECRET_ACCESS_KEY
#
# replication_access_key_id = ""
# replication_secret_access_key = ""

# =============================================================================
# Event Notifications Configuration (Enterprise Feature)
# =============================================================================
# Requires FeatureEvents license. Events are published when objects are
# created, deleted, or modified in buckets with notification configurations.
#
# Redis Pub/Sub Publisher
# Events are published to channels: {channel_prefix}:{bucket_name}
#
# [events]
# redis_addr = "localhost:6379"
# redis_password = ""
# redis_db = 0
# redis_channel = "s3:events"

# =============================================================================
# Federation Configuration (Community Feature)
# =============================================================================
# Enables S3 federation for connecting to external S3 buckets:
#   - Passthrough mode: Proxy requests to external S3, store metadata locally
#   - Migrating mode: Ingest data from external S3 into ZapFS (lazy + active)
#
# Use cases:
#   - Migration from AWS S3 or other S3-compatible storage
#   - Hybrid cloud with data spanning multiple S3 endpoints
#   - Gradual migration with zero downtime
#
[federation]
# Enable federation features (default: false)
enabled = false

# Instance mode determines what this metadata instance does:
#   - "api": Serve HTTP API only, enqueue migration tasks but don't process them
#   - "worker": Process migration tasks only, no HTTP API
#   - "both": Serve API and process tasks (suitable for smaller deployments)
mode = "both"

# Migration worker settings (only used in worker/both mode)
sync_batch_size = 1000      # Objects per batch when discovering from external S3
sync_concurrency = 5        # Number of concurrent migration workers
sync_rate_limit = 100       # Max objects/sec to sync (prevents external S3 throttling)

# External S3 client settings
external_timeout = "5m"           # Timeout for external S3 requests
external_max_idle_conns = 100     # Max idle connections to external S3

# Service features for federated buckets
# All disabled by default - defer to external S3 as source of truth
[federation.features]
lifecycle_enabled = false       # Process lifecycle rules for federated buckets locally
notifications_enabled = false   # Emit event notifications for federated buckets locally
access_logging_enabled = false  # Write access logs for federated buckets locally
metrics_enabled = false         # Collect metrics for federated buckets locally
