# ZapFS Helm Chart Values
# https://github.com/zapfs/zapfs
#
# This chart deploys ZapFS, a distributed object storage system with S3-compatible API.
#
# Quick Start:
#   helm install zapfs ./k8s/zapfs \
#     --set manager.iamMasterKey=$(openssl rand -base64 32) \
#     --set metadata.database.dsn="zapfs:password@tcp(mysql:3306)/zapfs"
#
# For DigitalOcean: see values-digitalocean.yaml
# For bare metal with high-performance networking: see comments on affinity settings

# =============================================================================
# Global Settings
# =============================================================================

nameOverride: ""
fullnameOverride: ""

image:
  repository: zapfs/zapfs
  pullPolicy: IfNotPresent
  tag: ""  # Defaults to Chart.appVersion

imagePullSecrets: []

serviceAccount:
  create: true
  automount: true
  annotations: {}
  name: ""

# =============================================================================
# Manager Cluster (Raft Consensus)
# =============================================================================
# The manager cluster provides:
# - Raft-based consensus for high availability
# - IAM credential management (central authority)
# - Service registry for file and metadata services
# - Placement decisions for chunk storage
# - Collection (bucket) management via Raft

manager:
  enabled: true
  replicas: 3  # Must be odd for Raft quorum (1, 3, 5)

  # Ports
  grpcPort: 8050    # gRPC API for clients and cluster operations
  raftPort: 8051    # Raft peer-to-peer communication
  debugPort: 8055   # Prometheus metrics, pprof, health checks
  adminPort: 8060   # IAM Admin HTTP API

  # Region identifier
  regionID: "default"

  # Storage for Raft logs and snapshots
  storage: "1Gi"
  storageClass: ""  # Use default storage class

  # IAM master key for encrypting secrets at rest
  # Generate with: openssl rand -base64 32
  # For production, use external secrets management
  iamMasterKey: ""
  iamMasterKeySecret: ""  # Or reference existing secret

  # Bootstrap configuration
  bootstrap:
    enabled: true  # Enable bootstrap job for cluster initialization

  # TLS configuration
  tls:
    enabled: false
    secretName: ""

  # Resource limits
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"

  # Pod placement
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}  # If empty, anti-affinity is applied automatically

  # Anti-affinity configuration (when affinity is empty)
  # - false (default): Soft anti-affinity (preferredDuringScheduling)
  #   Tries to spread across nodes but allows colocation if needed
  # - true: Hard anti-affinity (requiredDuringScheduling)
  #   Strictly requires different nodes - pods will stay Pending if not enough nodes
  antiAffinityStrict: false

  # Topology spread constraints for multi-zone/rack deployments
  # Example for zone spreading:
  # topologySpreadConstraints:
  #   - maxSkew: 1
  #     topologyKey: topology.kubernetes.io/zone
  #     whenUnsatisfiable: DoNotSchedule
  #     labelSelector:
  #       matchLabels:
  #         app.kubernetes.io/component: manager
  topologySpreadConstraints: []

  # PodDisruptionBudget configuration
  pdb:
    enabled: true
    # minAvailable: 2  # Or specify explicitly (default: maintains Raft quorum)
    # maxUnavailable: 1

  # Service configuration
  service:
    type: ClusterIP

  # Optional: mount config file
  config: false

# =============================================================================
# Metadata Service (S3 API Gateway)
# =============================================================================
# The metadata service provides:
# - S3-compatible HTTP API
# - AWS Signature V4/V2 authentication
# - Request routing and filtering
# - IAM credential sync from manager cluster
# - Bucket and object metadata storage (MySQL/SQLite)

metadata:
  enabled: true
  replicaCount: 2

  # Ports
  httpPort: 8082    # S3-compatible HTTP API
  grpcPort: 8083    # Internal gRPC
  debugPort: 8085   # Prometheus metrics, pprof, health checks

  # Manager cluster address for IAM sync
  managerAddr: ""  # Defaults to zapfs-manager:8050

  # Region identifier
  regionID: "us-west"

  # S3 domain names for virtual-hosted style bucket addressing
  # Requests to bucket.s3.example.com will be parsed as bucket "bucket"
  # Add your S3-compatible domain names here
  s3Domains:
    - "localhost"
    - "s3.local"
    # Example production domains:
    # - "s3.example.com"
    # - "s3.us-west-1.example.com"

  # Database configuration
  # Supports MySQL, Vitess, or SQLite
  # For Vitess: use vtgate address (e.g., "user:pass@tcp(vtgate:3306)/keyspace")
  database:
    driver: "mysql"  # mysql, sqlite
    # DSN format: user:password@tcp(host:port)/database
    # For Vitess/PlanetScale: same MySQL DSN format via vtgate
    dsn: ""  # Required - set via --set or values override
    # Example DSNs:
    #   MySQL:   "zapfs:password@tcp(mysql:3306)/zapfs"
    #   Vitess:  "zapfs:password@tcp(vtgate:3306)/zapfs@primary"
    #   SQLite:  "/data/metadata.db" (with driver: sqlite)

  # Rate limiting configuration
  rateLimit:
    enabled: true
    burstMultiplier: 2  # Allow 2x burst above rate limit

    # Redis for distributed rate limiting across multiple metadata replicas
    # Required when running multiple metadata servers to coordinate limits
    redis:
      enabled: false
      addr: "redis:6379"  # Redis service address
      password: ""
      db: 0
      poolSize: 10
      failOpen: true  # Allow requests when Redis is unavailable

  # TLS configuration
  tls:
    enabled: false
    secretName: ""

  # Resource limits
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "1000m"
      memory: "1Gi"

  # Pod placement
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}  # If empty, anti-affinity is applied automatically

  # Anti-affinity configuration (when affinity is empty)
  antiAffinityStrict: false

  # Topology spread constraints for multi-zone deployments
  topologySpreadConstraints: []

  # PodDisruptionBudget configuration
  pdb:
    enabled: true
    # minAvailable: 1  # Default: at least 1 available
    # maxUnavailable: 1

  # Service configuration
  service:
    type: ClusterIP

  # Ingress for external S3 access
  ingress:
    enabled: false
    className: ""
    annotations: {}
    hosts:
      - host: s3.example.com
        paths:
          - path: /
            pathType: Prefix
    tls: []

# =============================================================================
# File Servers (Chunk Storage)
# =============================================================================
# File servers provide:
# - Chunk storage and retrieval
# - Replication between file servers
# - Erasure coding support
# - Direct data path (bypasses metadata for reads)

file:
  enabled: true
  replicas: 2

  # Ports
  httpPort: 8080    # HTTP API for direct access
  grpcPort: 8081    # gRPC for internal communication
  debugPort: 8010   # Prometheus metrics, pprof, health checks

  # Storage for chunk data
  storage: "100Gi"
  storageClass: ""  # Use default storage class

  # TLS configuration
  tls:
    enabled: false
    secretName: ""

  # Resource limits
  resources:
    requests:
      cpu: "100m"
      memory: "512Mi"
    limits:
      cpu: "2000m"
      memory: "4Gi"

  # Pod placement
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}  # If empty, anti-affinity is applied automatically

  # Anti-affinity configuration (when affinity is empty)
  antiAffinityStrict: false

  # Colocation with manager nodes (for high-performance networking)
  # When true, file servers prefer scheduling on same nodes as managers.
  # Useful for bare metal deployments with:
  # - UCX/RDMA for low-latency inter-node communication
  # - NVMe-oF or other high-speed storage fabrics
  # - GPU-direct storage access
  # Note: This is a soft preference (weight 50) - anti-affinity still takes priority
  colocateWithManager: false

  # Topology spread constraints for multi-zone/rack deployments
  # Example for rack awareness (bare metal):
  # topologySpreadConstraints:
  #   - maxSkew: 1
  #     topologyKey: topology.kubernetes.io/zone  # or custom rack label
  #     whenUnsatisfiable: ScheduleAnyway
  #     labelSelector:
  #       matchLabels:
  #         app.kubernetes.io/component: file
  topologySpreadConstraints: []

  # PodDisruptionBudget configuration
  pdb:
    enabled: true
    # minAvailable: 1  # Default: at least 1 available
    # maxUnavailable: 1

  # Service configuration
  service:
    type: ClusterIP

# =============================================================================
# Federation Configuration (Community Feature)
# =============================================================================
# Enables S3 federation for connecting to external S3 buckets.
# Useful for:
# - Migration from AWS S3 or other S3-compatible storage
# - Hybrid cloud with data spanning multiple S3 endpoints
# - Gradual migration with zero downtime
#
# Federation modes:
# - passthrough: Proxy requests to external S3, store metadata locally
# - migrating: Ingest data from external S3 into ZapFS (lazy + active)

federation:
  # Enable federation features on metadata service
  enabled: false

  # API instances configuration
  # These serve S3 HTTP requests and can enqueue migration tasks
  api:
    # Mode for API instances: "api" (no workers) or "both" (API + workers)
    mode: "both"

  # Worker instances configuration
  # Dedicated instances for processing federation migration tasks
  workers:
    enabled: false           # Set to true to deploy dedicated worker instances
    replicaCount: 2          # Number of worker replicas
    mode: "worker"           # Worker-only mode (no HTTP API)

    # Worker-specific configuration
    syncConcurrency: 10      # Concurrent migration workers per pod
    syncRateLimit: 200       # Max objects/sec per pod
    syncBatchSize: 1000      # Objects per discovery batch

    # Resource limits for workers (typically need more than API servers)
    resources:
      requests:
        memory: "512Mi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "2000m"

    # Pod placement
    podAnnotations: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}

  # Shared federation settings (applied to all federation-enabled instances)
  externalTimeout: "5m"           # Timeout for external S3 requests
  externalMaxIdleConns: 100       # Max idle connections to external S3

  # Feature flags for federated buckets
  # All disabled by default - defer to external S3 as source of truth
  features:
    lifecycleEnabled: false       # Process lifecycle rules locally
    notificationsEnabled: false   # Emit event notifications locally
    accessLoggingEnabled: false   # Write access logs locally
    metricsEnabled: false         # Collect metrics locally

  # Autoscaling for worker instances
  # Scales based on migration task queue depth
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    # Target queue depth per worker
    targetQueueDepth: 100
    # Or use CPU-based scaling
    targetCPUUtilizationPercentage: 80

# =============================================================================
# External Database
# =============================================================================
# ZapFS requires an external SQL database for metadata storage.
# Supported databases: MySQL 8.0+, Vitess, PlanetScale, SQLite (dev only)
#
# For production, we recommend Vitess for horizontal scalability.
# See: https://vitess.io/
#
# Example Vitess deployment:
#   helm install vitess planetscale/vitess-operator
#
# Then configure metadata.database.dsn to point to vtgate.
